# -*- coding: utf-8 -*-
"""project_spam_final_BILSTM_GiuseppeGullo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vWlfGYZ_-8o_2kwG6yc7YcOTCsFC6kWU

# FINAL PROJECT - DEEP LEARNING PROJECT - “Spam/Ham messages detection and filtering” - BILSTM

## Preliminary steps

### Preliminary step 1: General Setup
"""

!pip install PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

!pip install keras-tuner

import pandas as pd
import re
import string
import numpy as np
import tensorflow as tf
import seaborn as sns

from  matplotlib import pyplot as plt

from tensorflow import keras
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from tensorflow.keras import layers
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional
from keras.models import Model
from keras_tuner.tuners import RandomSearch

from sklearn.feature_extraction import _stop_words
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score

"""### Preliminary step 2: Connection with Google Drive for upload our dataset

"""

"""
Authenticate and create the PyDrive client
"""
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

from google.colab import drive
drive.mount('/content/drive')

"""## Project steps

### Step 1: Collecting data (Data acquisition)
"""

df = pd.read_csv("drive/MyDrive/final_report_deeplearnig_PRDL_MLLB/progetto_nostro/spam_dataset.csv")
df

df['Category'].value_counts()

sns.countplot(x=df['Category'])

"""### Step 2-3: Preparing the data (Data pre-processing) + Building datasets (Data splitting and balancing)

#### **Cleaning the Raw Data and splitting**
"""

# Some auxiliary functions:

def remove_hyperlink(word):
    return  re.sub(r"http\S+", "", word)

def remove_number(word):
    result = re.sub(r'\d+', '', word)
    return result

def to_lower(word):
    result = word.lower()
    return result

def remove_punctuation(word):
    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))
    return result

def remove_whitespace(word):
    result = word.strip()
    return result

def replace_newline(word):
    return word.replace('\n','')

def clean_up_pipeline(sentence):
    cleaning_utils = [remove_hyperlink,
                      replace_newline,
                      to_lower,
                      remove_number,
                      remove_punctuation,remove_whitespace]
    for i in cleaning_utils:
        sentence = i(sentence)
    return sentence

emails_train, emails_test, target_train, target_test = train_test_split(df['Message'],df['Category'], test_size = 0.2)

x_train = [clean_up_pipeline(i) for i in emails_train]
x_test = [clean_up_pipeline(i) for i in emails_test]

print(emails_train.shape) #series
print(emails_train.tolist()[0])

print(len(x_train))#list
print(x_train[0])

"""#### **Tokenizing the Cleaned Data**"""

## some config values
EMBED_SIZE = 100 # how big is each word vector
MAX_FEATURE = 50000 # how many unique words to use (i.e num rows in embedding vector)
MAX_LEN = 2000 # max number of words in a question to use

tokenizer = Tokenizer(num_words=MAX_FEATURE)

tokenizer.fit_on_texts(x_train)

x_train_features = np.array(tokenizer.texts_to_sequences(x_train))
x_test_features = np.array(tokenizer.texts_to_sequences(x_test))

x_train_features[0]

"""#### **Text Sequencing: Padding**"""

x_train_features = pad_sequences(x_train_features,maxlen=MAX_LEN)
x_test_features = pad_sequences(x_test_features,maxlen=MAX_LEN)

x_train_features[0]

x_test_features[0]

"""#### **Label the encoding target variable**"""

le = LabelEncoder()
train_y = le.fit_transform(target_train.values)
test_y = le.fit_transform(target_test.values)

"""### Step 4: Choosing the best model (Model building)"""

# create the model
EMBEDDING_VECTOR_LENGTH = 32

model = tf.keras.Sequential()
model.add(Embedding(MAX_FEATURE, EMBEDDING_VECTOR_LENGTH, input_length=MAX_LEN))
model.add(Bidirectional(tf.keras.layers.LSTM(64)))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

"""### Step 5: Training the model (Model training)


"""

history = model.fit(x_train_features, train_y, batch_size=32, epochs=5, validation_data=(x_test_features, test_y))

"""### Step 6: Testing & Evaluating the model (Performance evaluation)

"""

y_predicted = model.predict(x_test_features)
y_predicted = y_predicted.flatten()

y_predicted

y_predicted = np.where(y_predicted > 0.5, 1, 0)

cf_matrix =confusion_matrix(test_y,y_predicted)

tn, fp, fn, tp = confusion_matrix(test_y,y_predicted).ravel()
print("Precision: {:.2f}%".format(100 * precision_score(test_y, y_predicted)))
print("Recall: {:.2f}%".format(100 * recall_score(test_y, y_predicted)))
print("F1 Score: {:.2f}%".format(100 * f1_score(test_y,y_predicted)))

f1_score(test_y,y_predicted)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.grid()
plt.show()

ax= plt.subplot()
sns.heatmap(cf_matrix, annot=True, ax = ax,cmap='Blues',fmt=''); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');
ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['Not Spam', 'Spam']); ax.yaxis.set_ticklabels(['Not Spam', 'Spam']);

"""### Step 7: Performing Hyperparameters Tuning


"""

def build_model(hp):
  model = tf.keras.Sequential()
  model.add(Embedding(MAX_FEATURE, EMBEDDING_VECTOR_LENGTH, input_length=MAX_LEN))
  model.add(Bidirectional(tf.keras.layers.LSTM(64)))
  model.add(Dense(
      hp.Choice('units', [8, 16, 32]),
      activation='relu'))
  model.add(Dropout(0.1))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(
       optimizer=keras.optimizers.Adam(
           hp.Choice ('learning_rate',
                      values=[0.01, 0.0001])),
                loss='binary_crossentropy',
                 metrics=['accuracy'])
  return model

tuner = RandomSearch(
    build_model,
    objective = 'val_accuracy',
    max_trials = 3,
    executions_per_trial = 2,
    overwrite = True
    )

tuner.search_space_summary()

tuner.search(x_train_features, train_y, epochs=5, validation_data=(x_test_features, test_y))

tuner.results_summary()

"""### Step 8: Interpreting the model results (Deployment)"""

def get_predictions(text):
  sequence = tokenizer.texts_to_sequences([text])
  # pad the sequence
  sequence = pad_sequences(sequence, maxlen=MAX_LEN)
  # get the prediction
  prediction = model.predict(sequence)[0]

  return prediction

messages = [
    'Enter a chance to win $5000, hurry up, offer valid until march 31, 2021',
    'You are awarded a SiPix Digital Camera! call 09061221061 from landline. Delivery within 28days. T Cs Box177. M221BP. 2yr warranty. 150ppm. 16 . p pÂ£3.99',
    'it to 80488. Your 500 free text messages are valid until 31 December 2005.',
    'Hey Sam, Are you coming for a cricket game tomorrow',
    "Why don't you wait 'til at least wednesday to see if you get your ."
]

for i in range(len(messages)):
  text = messages[i]
  print(text)
  pred = get_predictions(text)
  print(pred)
  if pred[0] >= 0.5:
    print("ALERT: It's a SPAM message!!!")
  else:
    print("It's a HAM message")
  print("\n")