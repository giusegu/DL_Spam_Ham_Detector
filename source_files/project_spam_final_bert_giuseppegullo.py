# -*- coding: utf-8 -*-
"""project_spam_final_BERT_GiuseppeGullo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KCdBvBS5t7muYS_HPbG17oSwCiRU0Gc9

# FINAL PROJECT - DEEP LEARNING PROJECT - “Spam/Ham messages detection and filtering” - BERT

## Preliminary steps

### Preliminary step 1: General Setup
"""

!pip install PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

!pip install keras-tuner
!pip install tensorflow_text

import pandas as pd
import re
import string
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
import seaborn as sns


from  matplotlib import pyplot as plt

from tensorflow import keras

from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from tensorflow.keras import layers
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional
from keras.models import Model
from keras_tuner.tuners import RandomSearch

from sklearn.feature_extraction import _stop_words
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score,classification_report
from sklearn.metrics.pairwise import cosine_similarity

"""### Preliminary step 2: Connection with Google Drive for upload our dataset

"""

"""
Authenticate and create the PyDrive client
"""
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

from google.colab import drive
drive.mount('/content/drive')

"""## Project steps

### Collecting data (Data acquisition)
"""

df = pd.read_csv("drive/MyDrive/final_report_deeplearnig_PRDL_MLLB/progetto_nostro/spam_dataset.csv")
df

df['Category'].value_counts()

sns.countplot(x=df['Category'])

"""####**Rebalancing of the dataset:**"""

df_spam = df[df['Category']=='spam']
df_spam.shape

df_ham = df[df['Category']=='ham']
df_ham.shape

df_ham_downsampled = df_ham.sample(df_spam.shape[0])
df_ham_downsampled.shape

df_balanced = pd.concat([df_ham_downsampled, df_spam])
df_balanced.shape

df_balanced['Category'].value_counts()

df_balanced['spam']=df_balanced['Category'].apply(lambda x: 1 if x=='spam' else 0)
df_balanced

"""### Preparing the data (Data pre-processing) + Building datasets (Data splitting and balancing)

#### **Splitting of the dataset**
"""

X_train, X_test, y_train, y_test = train_test_split(df_balanced['Message'],df_balanced['spam'], stratify=df_balanced['spam'])

X_train.head(4)

"""#### **BERT model to get embedding vectors**"""

bert_preprocess = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
bert_encoder = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4")

"""**Example of functioning of BERT**"""

def get_sentence_embeding(sentences):
    preprocessed_text = bert_preprocess(sentences)
    return bert_encoder(preprocessed_text)['pooled_output']

# Get embeding vectors for few sample words.

e = get_sentence_embeding([
    "banana",
    "grapes",
    "mango",
    "jeff bezos",
    "elon musk",
    "bill gates"
]
)

"""Compare them using cosine similarity: Values near to 1 means they are similar. 0 means they are very different. In fact, comparing "banana" and "grapes" you get 0.99 similarity because they both are fruits"""

cosine_similarity([e[0]],[e[1]])

"""### Choosing the best model (Model building)"""

METRICS = [
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall')
]

# create the model

# Bert layers
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)

# Neural network layers
l = tf.keras.layers.Dropout(0.1, name="dropout")(outputs['pooled_output'])
l = tf.keras.layers.Dense(1, activation='sigmoid', name="output")(l)

# Use inputs and outputs to construct a final model
model = tf.keras.Model(inputs=[text_input], outputs = [l])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=METRICS)

print(model.summary())

"""### Training the model (Model training)


"""

model.fit(X_train, y_train, epochs=10)

"""### Testing & Evaluating the model (Performance evaluation)

"""

model.evaluate(X_test, y_test)

y_predicted = model.predict(X_test)
y_predicted = y_predicted.flatten()

y_predicted = np.where(y_predicted > 0.5, 1, 0)
y_predicted

cm = confusion_matrix(y_test, y_predicted)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax,cmap='Blues',fmt=''); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');
ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['Not Spam', 'Spam']); ax.yaxis.set_ticklabels(['Not Spam', 'Spam']);

print(classification_report(y_test, y_predicted))

"""### Interpreting the model results (Deployment)"""

messages = [
    'Enter a chance to win $5000, hurry up, offer valid until march 31, 2021',
    'You are awarded a SiPix Digital Camera! call 09061221061 from landline. Delivery within 28days. T Cs Box177. M221BP. 2yr warranty. 150ppm. 16 . p pÂ£3.99',
    'it to 80488. Your 500 free text messages are valid until 31 December 2005.',
    'Hey Sam, Are you coming for a cricket game tomorrow',
    "Why don't you wait 'til at least wednesday to see if you get your ."
]
model.predict(messages)
